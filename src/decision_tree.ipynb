{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c8a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, MinMaxScaler, PowerTransformer, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, balanced_accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8534f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../db/monster_fights.csv')\n",
    "random_state = 37\n",
    "\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Check feature variances\n",
    "# print(X.describe())\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder_y = LabelEncoder()\n",
    "y = label_encoder_y.fit_transform(y)\n",
    "\n",
    "# Identify column types automatically\n",
    "float_cols = X.select_dtypes(include=['float'], exclude=['int']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category', 'int']).columns.tolist()\n",
    "\n",
    "## Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', PowerTransformer(method='yeo-johnson'), float_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # Safety for any unprocessed columns\n",
    ")\n",
    "\n",
    "## Create complete pipeline with Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=random_state))\n",
    "])\n",
    "\n",
    "## Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "## Basic model training\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Initial Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [10, 20, 40, 80, 100, 150, 200, 240, 300, 360, 400, 450], \n",
    "    'classifier__max_depth': [None, 5, 6, 7, 10, 13, 20, 30, 40],  \n",
    "    'classifier__min_samples_split': [0.01, 0.05, 0.1],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.3, 0.5, 0.8],  \n",
    "    'classifier__bootstrap': [True],\n",
    "    'classifier__class_weight': [None, 'balanced'],\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for faster tuning with many features\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    n_iter=300,\n",
    "    cv=7,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "def evaluate_model(search, X_test, y_test):\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nBest Params: {search.best_params_}\")\n",
    "    print(f\"CV Accuracy: {search.best_score_:.2f}\")\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        importances = best_model.named_steps['classifier'].feature_importances_\n",
    "        features = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        print(\"\\nTop 10 Features:\")\n",
    "        for feat, imp in sorted(zip(features, importances), \n",
    "                            key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"{feat}: {imp:.3f}\")\n",
    "\n",
    "evaluate_model(search, X_test, y_test)\n",
    "\n",
    "# # Get feature names after preprocessing\n",
    "# numeric_feature_names = numeric_features.tolist()\n",
    "# categorical_transformer = search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat']\n",
    "# categorical_feature_names = categorical_transformer.get_feature_names_out(categorical_features).tolist()\n",
    "# all_feature_names = numeric_feature_names + categorical_feature_names\n",
    "\n",
    "# # Get feature importances\n",
    "# importances = search.best_estimator_.named_steps['classifier'].feature_importances_\n",
    "# feature_importance = pd.DataFrame({'Feature': all_feature_names, 'Importance': importances})\n",
    "# feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "# # Plot top 15 features\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "# plt.title('Top 15 Feature Importances')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Evaluate on test set\n",
    "# best_model = search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# print(\"\\nTest Set Evaluation:\")\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3724ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found:\n",
      "{'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "\n",
      "Test Accuracy: 0.7623\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.61      0.72        28\n",
      "           1       0.78      0.64      0.70        11\n",
      "           2       0.73      0.89      0.80        54\n",
      "           3       0.75      0.72      0.74        29\n",
      "\n",
      "    accuracy                           0.76       122\n",
      "   macro avg       0.79      0.71      0.74       122\n",
      "weighted avg       0.78      0.76      0.76       122\n",
      "\n",
      "CV F1: 0.71 ± 0.04\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('../db/monster_fights.csv')\n",
    "random_state = 42\n",
    "\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Identify column types\n",
    "float_cols = X.select_dtypes(include=['float']).columns.tolist()\n",
    "bool_cols = [col for col in X.columns \n",
    "            if set(X[col].unique()).issubset({0, 1})]\n",
    "object_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Convert boolean columns to int (0/1)\n",
    "X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "# One-hot encode categorical (object) columns\n",
    "if object_cols:\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    object_encoded = ohe.fit_transform(X[object_cols])\n",
    "    object_encoded_df = pd.DataFrame(object_encoded,\n",
    "                                columns=ohe.get_feature_names_out(object_cols),\n",
    "                                index=X.index)\n",
    "    X = pd.concat([X.drop(object_cols, axis=1), object_encoded_df], axis=1)\n",
    "\n",
    "# Convert target\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "# --- Hyperparameter Tuning ---\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 75, 100, 125, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize base model\n",
    "base_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    objective='multi:softprob' if len(np.unique(y)) > 2 else 'binary:logistic',\n",
    "    eval_metric='mlogloss' if len(np.unique(y)) > 2 else 'logloss',\n",
    "    random_state=random_state,\n",
    "    early_stopping_rounds=10,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    \n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    verbose=False  # Show detailed progress\n",
    ")\n",
    "\n",
    "# Run grid search (using eval_set for early stopping)\n",
    "grid_search.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False  # Disables duplicate progress bars\n",
    ")\n",
    "\n",
    "# --- Results ---\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Convert to numpy arrays if they aren't already\n",
    "X_array = X.values if hasattr(X, 'values') else X\n",
    "y_array = y.values if hasattr(y, 'values') else y\n",
    "\n",
    "# Initialize CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = []\n",
    "\n",
    "# Manual cross-validation loop\n",
    "for train_idx, val_idx in cv.split(X_array, y_array):\n",
    "    X_train, X_val = X_array[train_idx], X_array[val_idx]\n",
    "    y_train, y_val = y_array[train_idx], y_array[val_idx]\n",
    "    \n",
    "    # Train with early stopping\n",
    "    model = XGBClassifier(\n",
    "        **grid_search.best_params_,  # Your tuned parameters\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predict and score\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1_scores.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "\n",
    "print(f\"CV F1: {np.mean(f1_scores):.2f} ± {np.std(f1_scores):.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plot_importance(best_model, max_num_features=20)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
